{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Face Mask Detection VGG16+SSDResNet10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ivanhartono/face-mask/blob/main/Face_Mask_Detection_VGG16%2BSSDResNet10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44JRqlK3IyFv"
      },
      "source": [
        "**Face Mask Detection**\n",
        "\n",
        "Program Machine Learning untuk mendeteksi penggunaan masker. Program dibuat menggunakan metode CNN dengan arsitektur VGG16Net dan caffe model ResNet-10 untuk face detection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NJ4ryhAKp2X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3338cf56-5dd7-4d6d-d4ac-1f50bf26847e"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Jun  5 11:29:09 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   65C    P0    29W /  70W |   9008MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "time: 171 ms (started: 2021-06-05 11:29:09 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xR_DdkBiKsp-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "461d1a93-b067-4d55-8cac-1c68bfcc778d"
      },
      "source": [
        "# Memeriksa CUDA\n",
        "!/usr/local/cuda/bin/nvcc --version"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Wed_Jul_22_19:09:09_PDT_2020\n",
            "Cuda compilation tools, release 11.0, V11.0.221\n",
            "Build cuda_11.0_bu.TC445_37.28845127_0\n",
            "time: 184 ms (started: 2021-06-05 11:29:10 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3ph4kTgu8UN"
      },
      "source": [
        "## Tahap Awal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZ4nspthbm3V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fe2bd0c-8b82-4bb7-91c8-573dc6b0f850"
      },
      "source": [
        "# Melakukan cloning data\n",
        "!git clone https://github.com/ivanhartono/face-mask.git"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'face-mask'...\n",
            "remote: Enumerating objects: 3772, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 3772 (delta 2), reused 2 (delta 1), pack-reused 3765\u001b[K\n",
            "Receiving objects: 100% (3772/3772), 164.77 MiB | 35.84 MiB/s, done.\n",
            "Resolving deltas: 100% (5/5), done.\n",
            "time: 6.49 s (started: 2021-06-05 11:55:34 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGAocb4i3q_R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "765cfe4f-0e7e-482e-bcd9-959fbe3f105f"
      },
      "source": [
        "# Berpindah ke folder face-mask-detection\n",
        "%cd face-mask/\n",
        "\n",
        "# Memeriksa isi folder face-mask-detection\n",
        "!ls"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/face-mask/face-mask/face-mask/face-mask/face-mask-detection/face-mask/face-mask\n",
            "dataset\t\t   Face_Mask_Detection_VGG16Net_with_ResNet10.ipynb\n",
            "face-detector.zip  mask_model.tflite\n",
            "time: 232 ms (started: 2021-06-05 11:55:42 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnqMpfEwL_Zp"
      },
      "source": [
        "**Face-detector.zip** merupakan file untuk melakukan pengujian model dengan menggunakan res10_300x300_ssd_iter_140000.caffemodel dan deploy.prototxt untuk mendeteksi bagian wajah"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYnQVobFGH7S",
        "outputId": "b23057a8-05df-4622-bcac-c13e54a4f921"
      },
      "source": [
        "# Melakukan unzip pada folder face-detector.zip\n",
        "!unzip face-detector.zip"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  face-detector.zip\n",
            "   creating: face-detector/\n",
            "  inflating: face-detector/deploy.prototxt  \n",
            "   creating: face-detector/example_img/\n",
            "  inflating: face-detector/example_img/ex01.jpg  \n",
            "  inflating: face-detector/example_img/ex02.jpg  \n",
            "  inflating: face-detector/example_img/ex03.jpg  \n",
            "  inflating: face-detector/example_img/ex04.jpg  \n",
            "  inflating: face-detector/example_img/ex05.jpg  \n",
            "  inflating: face-detector/example_img/ex06.jpg  \n",
            "  inflating: face-detector/example_img/ex07.jpg  \n",
            "  inflating: face-detector/res10_300x300_ssd_iter_140000.caffemodel  \n",
            "time: 313 ms (started: 2021-06-05 11:55:45 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BvjSZ30YhWh"
      },
      "source": [
        "## Mengimpor Libraries yang dibutuhkan"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3Za-KOKf-Ot",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db1d6058-32cb-451c-ac23-9cbee120c0aa"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import vgg16\n",
        "from tensorflow.keras.layers import AveragePooling2D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from imutils import paths\n",
        "import matplotlib.pyplot as plt\n",
        "import pathlib\n",
        "import numpy as np\n",
        "import argparse\n",
        "import os\n",
        "import itertools\n",
        " \n",
        "# Mengihitung waktu lamanya eksekusi tiap sel di Google Colab\n",
        "!pip install ipython-autotime\n",
        " \n",
        "%load_ext autotime"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ipython-autotime in /usr/local/lib/python3.7/dist-packages (0.3.1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from ipython-autotime) (5.5.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.7.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (2.6.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (5.0.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (57.0.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (1.0.18)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.8.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.8.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->ipython->ipython-autotime) (0.2.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->ipython-autotime) (0.7.0)\n",
            "The autotime extension is already loaded. To reload it, use:\n",
            "  %reload_ext autotime\n",
            "time: 2.38 s (started: 2021-06-05 11:29:18 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-AQ5m0DgnW1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "804cbfbc-c620-454f-9d4b-40cec42011d1"
      },
      "source": [
        "# Memeriksa Versi TensorFlow\n",
        "print(tf.__version__)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.5.0\n",
            "time: 829 µs (started: 2021-06-05 11:29:21 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZU5Fqk2vLELg"
      },
      "source": [
        "## Preprocessing Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D48VICs21pna",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8762ab8-bad8-4f2b-d39b-c7b594dce7a1"
      },
      "source": [
        "# Inisialisasi nilai Initial Learning Rate, berapa banyak Epoch pelatihan, dan Batch Size\n",
        "INIT_LR = 1e-4\n",
        "EPOCHS = 30\n",
        "BS = 32\n",
        " \n",
        "# Mengambil gambar dari dataset directory, kemudian inisialisasi data dan class gambar\n",
        "print(\"Menginput gambar...\")\n",
        "imagePaths = list(paths.list_images('dataset'))\n",
        "data = []\n",
        "labels = []\n",
        " \n",
        "# Melakukan perulangan pada image paths\n",
        "for imagePath in imagePaths:\n",
        " \n",
        "    # Mengekstrak class label dari filename\n",
        "    label = imagePath.split(os.path.sep)[-2]\n",
        "    # Memuat input gambar (224x224) dan melakukan proses\n",
        "    image = load_img(imagePath, target_size=(224, 224))\n",
        "    image = img_to_array(image)\n",
        "    image = preprocess_input(image)\n",
        " \n",
        "    # Mengupdate data dan labels lists, berurutan\n",
        "    data.append(image)\n",
        "    labels.append(label)\n",
        " \n",
        "# Mengkonversi data dan label ke dalam NumPy Arrays\n",
        "data = np.array(data, dtype=\"float32\")\n",
        "labels = np.array(labels)\n",
        " \n",
        "# Melakukan one-hot encoding on the labels\n",
        "lb = LabelBinarizer()\n",
        "labels = lb.fit_transform(labels)\n",
        "labels = to_categorical(labels)\n",
        "print(\"Input gambar berhasil\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Menginput gambar...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Input gambar berhasil\n",
            "time: 18.6 s (started: 2021-06-05 11:29:21 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aF2vfrgni3tq"
      },
      "source": [
        "### Membuat objek ImageDataGenerator dan Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRBrygye5yvc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb443984-187f-4b78-c6de-ad27433bdc38"
      },
      "source": [
        "# Mempartisi data ke dalam pelatihan dan pengujian ( 75% : 25% )\n",
        "(trainX, testX, trainY, testY) = train_test_split(data, labels,\n",
        "    test_size=0.20, stratify=labels, random_state=42)\n",
        " \n",
        "# Membentuk training image generator untuk data augmentation\n",
        "aug = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.15,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.15,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode=\"nearest\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 1.29 s (started: 2021-06-05 11:29:39 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHPD753f5205"
      },
      "source": [
        "## Membuat Model Jaringan CNN yang sudah dipelajari sebelumnya (pre-trained convnets)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaDceTqP6HLr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ffb986f-3588-42fc-c2b2-6eb6d007cc73"
      },
      "source": [
        "# Arsitektur jaringan VGG16Net\n",
        "baseModel = tf.keras.applications.VGG16(weights=\"imagenet\", include_top=False,\n",
        "    input_tensor=Input(shape=(224, 224, 3)))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 1.76 s (started: 2021-06-05 11:29:41 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfSLr2q2LWRZ"
      },
      "source": [
        "### Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0U2qYnO6KYW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f02561e-a906-4f03-a443-0647095e3216"
      },
      "source": [
        "baseModel.trainable = False\n",
        "baseModel.summary()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 0\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n",
            "time: 7.49 ms (started: 2021-06-05 11:29:42 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENpg9P6Q6SpL"
      },
      "source": [
        "## Tahap Pembuatan Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXEyXB5NTU13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6e49fe2-10d5-44b8-b270-7881effbd03a"
      },
      "source": [
        "# Membentuk bagian head dari model yang akan ditempatkan pada base model\n",
        "headModel = baseModel.output\n",
        "headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\n",
        "headModel = Flatten(name=\"flatten\")(headModel)\n",
        "headModel = Dense(128, activation=\"relu\")(headModel)\n",
        "headModel = Dropout(0.5)(headModel)\n",
        "headModel = Dense(2, activation=\"softmax\")(headModel)\n",
        " \n",
        "# Menempatkan head model pada base model\n",
        "model = Model(inputs=baseModel.input, outputs=headModel)\n",
        " \n",
        "# Perulangan pada seluruh base model\n",
        "for layer in baseModel.layers:\n",
        "    layer.trainable = False\n",
        " \n",
        "# Persiapan kompilasi model\n",
        "print(\"Mengkompilasi model...\")\n",
        "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=opt,\n",
        "    metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mengkompilasi model...\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d_1 (Average (None, 1, 1, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 2)                 258       \n",
            "=================================================================\n",
            "Total params: 14,780,610\n",
            "Trainable params: 65,922\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n",
            "time: 54.7 ms (started: 2021-06-05 11:29:42 +00:00)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lH42Hp3UWrPs"
      },
      "source": [
        "### Melakukan Pelatihan Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZPSj-se6WdI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "outputId": "8d22991c-bc1d-4c1e-ea27-b3ff089994c2"
      },
      "source": [
        "# Pelatihan model\n",
        "print(\"Training head model...\")\n",
        "H = model.fit(\n",
        "    aug.flow(trainX, trainY, batch_size=BS),\n",
        "    steps_per_epoch=len(trainX) // BS,\n",
        "    validation_data=(testX, testY),\n",
        "    validation_steps=len(testX) // BS,\n",
        "    epochs=EPOCHS)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training head model...\n",
            "Epoch 1/30\n",
            "95/95 [==============================] - ETA: 0s - loss: 1.1212 - accuracy: 0.7179"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-f0ba105336c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mBS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     epochs=EPOCHS)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1223\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m               \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1225\u001b[0;31m               _use_cached_eval_dataset=True)\n\u001b[0m\u001b[1;32m   1226\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1487\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1489\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1490\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1491\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    922\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "time: 50.1 s (started: 2021-06-05 11:29:42 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxqTwaGi7Wzs"
      },
      "source": [
        "## Menampilkan Grafik Model Hasil Pelatihan"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OURpwhV87ac9"
      },
      "source": [
        "N = EPOCHS\n",
        "fig = plt.figure(figsize=(7, 4))\n",
        "fig.set_figheight(10)\n",
        "fig.set_figwidth(15)\n",
        " \n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(np.arange(0, N), H.history[\"accuracy\"],label = \"Training Accuracy\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_accuracy\"],label = \"Validation Accuracy\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Akurasi\")\n",
        "plt.title(\"Kurva Tingkat Akurasi\", size=15)\n",
        "plt.grid(zorder = 0)\n",
        " \n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(np.arange(0, N), H.history[\"loss\"],label = \"Training Loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_loss\"],label = \"Validation Loss\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Error\")\n",
        "plt.title(\"Kurva Tingkat Error\", size=15)\n",
        "plt.grid(zorder = 0)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Mqe8nNB7lp8"
      },
      "source": [
        "## Evaluasi Jaringan"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLxROO1pg7Em"
      },
      "source": [
        "# Memeriksa matriks model\n",
        "print(model.metrics_names)\n",
        "# Evaluasi data test\n",
        "print(model.evaluate(x= testX, y = testY))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySJujhciZvlQ"
      },
      "source": [
        "# Menampilkan matriks yang benar dan matriks hasil prediksi\n",
        "# Label yang benar\n",
        "yTrue = np.argmax(testY, axis=1)\n",
        "\n",
        "# Label prediksi\n",
        "YPred = model.predict(testX, batch_size=BS)\n",
        "yPred = np.argmax(YPred, axis=1)\n",
        "\n",
        "print(yTrue)\n",
        "print(yPred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yl0Y-D41imCP"
      },
      "source": [
        "## Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EL_ROzJdh6pm"
      },
      "source": [
        "def get_confusion_matrix(yTrue, yPred):\n",
        "    n_classes = len(np.unique(yTrue)) \n",
        "    conf = np.zeros((n_classes, n_classes))\n",
        "    for actual, pred in zip(yTrue, yPred):\n",
        "        conf[int(actual)][int(pred)] += 1\n",
        "    return conf.astype('int')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxoC6xxjaCrf"
      },
      "source": [
        "conf = get_confusion_matrix(yTrue, yPred)\n",
        "conf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ss9YmprkdirH"
      },
      "source": [
        "classes = [0, 1]\n",
        "# Plot confusion matrix\n",
        "plt.imshow(conf, interpolation='nearest', cmap=plt.cm.Greens)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(len(classes))\n",
        "plt.xticks(tick_marks, classes)\n",
        "plt.yticks(tick_marks, classes)\n",
        "\n",
        "fmt = 'd'\n",
        "thresh = conf.max() / 2.\n",
        "for i, j in itertools.product(range(conf.shape[0]), range(conf.shape[1])):\n",
        "    plt.text(j, i, format(conf[i, j], fmt),\n",
        "             horizontalalignment=\"center\",\n",
        "             color=\"white\" if conf[i, j] > thresh else \"black\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.ylabel('Label benar')\n",
        "plt.xlabel('Label Prediksi')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sr58aW7Ui7PD"
      },
      "source": [
        "Analisis mAP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Psuc1y0i_NJ"
      },
      "source": [
        "# Berdasarkan confusion matrix\n",
        "TP = true_pos = 378\n",
        "TN = true_neg = 372\n",
        "FP = false_pos = 11\n",
        "FN = false_neg = 6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YorxeJVtj0tZ"
      },
      "source": [
        "results = {}\n",
        "\n",
        "# Akurasi\n",
        "metric = \"Akurasi\"\n",
        "results[metric] = (TP + TN) / (TP + TN + FP + FN)\n",
        "print(f\"{metric} = {results[metric]: .3f}\")\n",
        "\n",
        "# Recall\n",
        "metric = \"Recall\"\n",
        "results[metric] = TP / (TP + FN)\n",
        "print(f\"{metric} = {results[metric]: .3f}\")\n",
        "\n",
        "# Presisi\n",
        "metric = \"Presisi\"\n",
        "results[metric] = TP / (TP + FP)\n",
        "print(f\"{metric} = {results[metric]: .3f}\")\n",
        "\n",
        "# Nilai F1\n",
        "metric = \"F1\"\n",
        "results[metric] = 2 / (1 / results[\"Presisi\"] + 1 / results[\"Recall\"])\n",
        "print(f\"{metric} = {results[metric]: .3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDPEy5zamOoi"
      },
      "source": [
        "# Membuat prediksi dari pengujian\n",
        "predIdxs = model.predict(testX, batch_size=BS)\n",
        " \n",
        "# Untuk setiap gambar dalam set pengujian, kita perlu menemukan indeks label\n",
        "# dengan probabilitas prediksi terbesar\n",
        "predIdxs = np.argmax(predIdxs, axis=1)\n",
        " \n",
        "# Menampilkan laporan klasifikasi yang diformat dengan baik\n",
        "print(classification_report(testY.argmax(axis=1), predIdxs,\n",
        "    target_names=lb.classes_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8U7B1jv8ZQr"
      },
      "source": [
        "## Menyimpan dan Konversi Model ke \".tflite\"\n",
        "Menyimpan model menggunakan tf.saved_model/save dan kemudian mengkonversi model tersimpan ke format yang kompatibel tf lite."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTaHZm7mKjLK"
      },
      "source": [
        "export_dir='saved_model/1'\n",
        "tf.saved_model.save(model, export_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQDDJ2eYKk21"
      },
      "source": [
        "# Mengkonvert model ke format tflite\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)\n",
        "tflite_model = converter.convert()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZet8SWWKn_S"
      },
      "source": [
        "# Menyimpan model\n",
        "tflite_model_file = pathlib.Path('model.tflite')\n",
        "tflite_model_file.write_bytes(tflite_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znn1rBf6Jcfu"
      },
      "source": [
        "# Memuat model dan mengalokasikan ke tensor\n",
        "interpreter = tf.lite.Interpreter(model_content = tflite_model)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Mendapatkan input dan ouput tensor\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "input_shape = input_details[0]['shape']\n",
        "\n",
        "print(input_details)\n",
        "print(output_details)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-H94fpq_E0J"
      },
      "source": [
        "# Pengujian Model dengan SSD ResNet10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9T0GaCA5_SRw"
      },
      "source": [
        "Model diujikan pada gambar dan secara real-time dengan menggunakan res10_300x300_ssd_iter_14000.caffemodel dan deploy.prototxt yang digunakan untuk mendeteksi wajah."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nej9QmRT8-DO"
      },
      "source": [
        "## Penggunaan Model pada Gambar\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwPKPee0Qi3h"
      },
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkkXJ-MpKKSV"
      },
      "source": [
        "image = cv2.imread('/content/face-mask-detection/example_img/ex07.jpg')\n",
        "orig = image.copy()\n",
        "(h, w) = image.shape[:2]\n",
        "\n",
        "blob = cv2.dnn.blobFromImage(image, 1.0, (300, 300),\n",
        "\t(104.0, 177.0, 123.0))\n",
        "\n",
        "net=cv2.dnn.readNet('/content/face-mask-detection/deploy.prototxt','/content/face-mask-detection/res10_300x300_ssd_iter_140000.caffemodel')\n",
        "\n",
        "# Melewatkan blob melalui jaringan dan mendapatkan deteksi wajah\n",
        "print(\"Mendeteksi wajah...\")\n",
        "net.setInput(blob)\n",
        "detections = net.forward()\n",
        "\n",
        "for i in range(0, detections.shape[2]):\n",
        "\t# ekstrak keyakinan (yaitu, probabilitas) yang terkait dengan deteksi\n",
        "\tconfidence = detections[0, 0, i, 2]\n",
        "\n",
        "\tif confidence > 0.5:\n",
        "\t\t# Menghitung koordinat (x, y) dari kotak pembatas untuk objek\n",
        "\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n",
        "\n",
        "\t\t# Memastikan kotak pembatas berada dalam dimensi bingkai\n",
        "\t\t(startX, startY) = (max(0, startX), max(0, startY))\n",
        "\t\t(endX, endY) = (min(w - 1, endX), min(h - 1, endY))\n",
        "\n",
        "\t\t# Ekstrak ROI wajah, konversikan dari BGR ke pemesanan saluran RGB,\n",
        "    # dan mengubah ukurannya menjadi 224x224, dan lalu pre-proses\n",
        "\t\tface = image[startY:endY, startX:endX]\n",
        "\t\tface = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
        "\t\tface = cv2.resize(face, (224, 224))\n",
        "\t\tface = img_to_array(face)\n",
        "\t\tface = preprocess_input(face)\n",
        "\t\tface = np.expand_dims(face, axis=0)\n",
        "\n",
        "\t\t# Membaca wajah dengan model\n",
        "\t\t(mask, withoutMask) = model.predict(face)[0]\n",
        "\n",
        "\t\t# Menggunakan masker hijau, tidak bermasker merah\n",
        "\t\tlabel = \"Bermasker\" if mask > withoutMask else \"Tidak Bermasker\"\n",
        "\t\tcolor = (0, 255, 0) if label == \"Bermasker\" else (0, 0, 255)\n",
        "\n",
        "\t\t# Probabilitas hasil deteksi\n",
        "\t\tlabel = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100)\n",
        "\n",
        "\t\t# Menampilkan hasil dengan label dan kotak\n",
        "\t\tcv2.putText(image, label, (startX, startY - 10),\n",
        "\t\t\tcv2.FONT_HERSHEY_TRIPLEX, 0.45, color, 2)\n",
        "\t\tcv2.rectangle(image, (startX, startY), (endX, endY), color, 2)\n",
        "\n",
        "# Menampilkan output\n",
        "cv2_imshow(image)\n",
        "cv2.waitKey(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njXIK_L3dVXx"
      },
      "source": [
        "## Pengujian Deteksi Perframe Capture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wdWN4xy-Nxd"
      },
      "source": [
        "# Mengimport lib\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.models import load_model\n",
        "from imutils.video import VideoStream\n",
        "import numpy as np\n",
        "import argparse\n",
        "import imutils\n",
        "import time\n",
        "import cv2\n",
        "import os\n",
        " \n",
        "def detect_and_predict_mask(frame, faceNet, maskNet):\n",
        "    # Membuat dimensi kotak deteksi\n",
        "    (h, w) = frame.shape[:2]\n",
        "    blob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300),\n",
        "        (104.0, 177.0, 123.0))\n",
        " \n",
        "    # Melewatkan blob dan mendeteksi wajah\n",
        "    faceNet.setInput(blob)\n",
        "    detections = faceNet.forward()\n",
        " \n",
        "    # Inisialisasi\n",
        "    faces = []\n",
        "    locs = []\n",
        "    preds = []\n",
        " \n",
        "    for i in range(0, detections.shape[2]):\n",
        "        # ekstrak keyakinan (yaitu, probabilitas) yang terkait dengan deteksi\n",
        "        confidence = detections[0, 0, i, 2]\n",
        " \n",
        "        if confidence > 0.5:\n",
        "            # Menghitung koordinat (x, y) dari kotak pembatas untuk objek\n",
        "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
        " \n",
        "            # Memastikan kotak pembatas berada dalam dimensi bingkai\n",
        "            (startX, startY) = (max(0, startX), max(0, startY))\n",
        "            (endX, endY) = (min(w - 1, endX), min(h - 1, endY))\n",
        " \n",
        "            # Ekstrak ROI wajah, konversikan dari BGR ke pemesanan saluran RGB,\n",
        "            # dan mengubah ukurannya menjadi 224x224, dan lalu pre-proses\n",
        "            face = frame[startY:endY, startX:endX]\n",
        "            face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
        "            face = cv2.resize(face, (224, 224))\n",
        "            face = img_to_array(face)\n",
        "            face = preprocess_input(face)\n",
        " \n",
        "            # Menambahkan kotak deteksi\n",
        "            faces.append(face)\n",
        "            locs.append((startX, startY, endX, endY))\n",
        " \n",
        "    if len(faces) > 0:\n",
        "        faces = np.array(faces, dtype=\"float32\")\n",
        "        preds = maskNet.predict(faces, batch_size=32)\n",
        " \n",
        "    return (locs, preds)\n",
        " \n",
        "faceNet=cv2.dnn.readNet('/content/face-mask-detection/deploy.prototxt','/content/face-mask-detection/res10_300x300_ssd_iter_140000.caffemodel')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ajg0vfLNs6C"
      },
      "source": [
        "import base64\n",
        "import html\n",
        "import io\n",
        "import time\n",
        " \n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2\n",
        " \n",
        "def start_input():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "    \n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "    \n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "    \n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 512, 512);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "      \n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "           \n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "      \n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML = \n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'Ketika selesai, klik disini atau pada video untuk berhenti dari demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "      \n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 512; //video.videoWidth;\n",
        "      captureCanvas.height = 512; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "      return stream;\n",
        "    }\n",
        "    async function takePhoto(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "      \n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "            \n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      \n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "      \n",
        "      return {'create': preShow - preCreate, \n",
        "              'show': preCapture - preShow, \n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        " \n",
        "  display(js)\n",
        "  \n",
        "def take_photo(label, img_data):\n",
        "  data = eval_js('takePhoto(\"{}\", \"{}\")'.format(label, img_data))\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvzQUeMGN5XQ"
      },
      "source": [
        "def js_reply_to_image(js_reply):\n",
        "    \"\"\"\n",
        "    input: \n",
        "          js_reply: JavaScript object, contain image from webcam\n",
        "    output: \n",
        "          image_array: image array RGB size 512 x 512 from webcam\n",
        "    \"\"\"\n",
        "    jpeg_bytes = base64.b64decode(js_reply['img'].split(',')[1])\n",
        "    image_PIL = Image.open(io.BytesIO(jpeg_bytes))\n",
        "    image_array = np.array(image_PIL)\n",
        " \n",
        "    return image_array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETwcarZEN7Np"
      },
      "source": [
        "start_input()\n",
        "label_html = 'Capturing...'\n",
        "img_data = ''\n",
        "count = 0 \n",
        "from google.colab.patches import cv2_imshow\n",
        "while True:\n",
        "  js_reply = take_photo(label_html, img_data)\n",
        "  if not js_reply:\n",
        "    break\n",
        "    \n",
        "  image = js_reply_to_image(js_reply)\n",
        "\n",
        "\t# Mengambil frame dari aliran video berulir dan \n",
        "  # ukurannya maksimum lebar 400 pixel\n",
        "  frame = image\n",
        "  v=True\n",
        "  if v == True:\n",
        "\n",
        "    frame = imutils.resize(frame, width=400)\n",
        "\n",
        "\t# Mendeteksi bermasker atau tidak\n",
        "    (locs, preds) = detect_and_predict_mask(frame, faceNet, model)\n",
        "    for (box, pred) in zip(locs, preds):\n",
        "\n",
        "\n",
        "\t\t# Membuka kotak dan prediksi\n",
        "      (startX, startY, endX, endY) = box\n",
        "      (mask, withoutMask) = pred\n",
        "\n",
        "\t\t# Menggunakan masker hijau, tidak bermasker merah\n",
        "      label = \"Bermasker\" if mask > withoutMask else \"Tidak Bermasker\"\n",
        "      color = (0, 255, 0) if label == \"Bermasker\" else (0, 0, 255)\n",
        "\n",
        "\t\t# Probabilitas pada label\n",
        "      label = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100)\n",
        "\n",
        "\t\t# Menampilkan hasil dengan label dan kotak dari frame\n",
        "      frame=cv2.putText(frame, label, (startX, startY - 10),cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
        "      frame=cv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n",
        "\n",
        "\t# Menampilkan ouput\n",
        "      cv2_imshow(frame)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6ziSC8bK9zR"
      },
      "source": [
        "## Pengujian Streaming Video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqoA42eicSdG"
      },
      "source": [
        "# import dependencies\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import cv2\n",
        "import numpy as np\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Id8nYgQTDmw"
      },
      "source": [
        "# function to convert the JavaScript object into an OpenCV image\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BGR image\n",
        "  \"\"\"\n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img\n",
        "\n",
        "# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "  # convert array into PIL image\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # format bbox into png for return\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  # format return string\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXrV4AMILGfb"
      },
      "source": [
        "# JavaScript to properly create our live video stream using our webcam as input\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "    \n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "    \n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "    \n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 512, 512);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "      \n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "           \n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "      \n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML = \n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "      \n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 512; //video.videoWidth;\n",
        "      captureCanvas.height = 512; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "      \n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "            \n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      \n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "      \n",
        "      return {'create': preShow - preCreate, \n",
        "              'show': preCapture - preShow, \n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "  \n",
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8A4FPtbLUA3"
      },
      "source": [
        " def detect_and_predict_mask(img, faceNet, maskNet):\n",
        "    # Membuat dimensi kotak deteksi\n",
        "    (h, w) = img.shape[:2]\n",
        "    blob = cv2.dnn.blobFromImage(img, 1.0, (300, 300),\n",
        "        (104.0, 177.0, 123.0))\n",
        " \n",
        "    # Melewatkan blob dan mendeteksi wajah\n",
        "    faceNet.setInput(blob)\n",
        "    detections = faceNet.forward()\n",
        " \n",
        "    # Inisialisasi\n",
        "    faces = []\n",
        "    locs = []\n",
        "    preds = []\n",
        " \n",
        "    for i in range(0, detections.shape[2]):\n",
        "        # ekstrak keyakinan (yaitu, probabilitas) yang terkait dengan deteksi\n",
        "        confidence = detections[0, 0, i, 2]\n",
        " \n",
        "        if confidence > 0.5:\n",
        "            # Menghitung koordinat (x, y) dari kotak pembatas untuk objek\n",
        "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
        " \n",
        "            # Memastikan kotak pembatas berada dalam dimensi bingkai\n",
        "            (startX, startY) = (max(0, startX), max(0, startY))\n",
        "            (endX, endY) = (min(w - 1, endX), min(h - 1, endY))\n",
        " \n",
        "            # Ekstrak ROI wajah, konversikan dari BGR ke pemesanan saluran RGB,\n",
        "            # dan mengubah ukurannya menjadi 224x224, dan lalu pre-proses\n",
        "            face = frame[startY:endY, startX:endX]\n",
        "            face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
        "            face = cv2.resize(face, (224, 224))\n",
        "            face = img_to_array(face)\n",
        "            face = preprocess_input(face)\n",
        " \n",
        "            # Menambahkan kotak deteksi\n",
        "            faces.append(face)\n",
        "            locs.append((startX, startY, endX, endY))\n",
        " \n",
        "    if len(faces) > 0:\n",
        "        faces = np.array(faces, dtype=\"float32\")\n",
        "        preds = maskNet.predict(faces, batch_size=32)\n",
        " \n",
        "    return (locs, preds)\n",
        " \n",
        "faceNet=cv2.dnn.readNet('/content/face-mask-detection/deploy.prototxt','/content/face-mask-detection/res10_300x300_ssd_iter_140000.caffemodel')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9whfB0urLIpj"
      },
      "source": [
        "# Memulai streaming video dari webcam\n",
        "video_stream()\n",
        "# Label untuk video\n",
        "label_html = 'Capturing...'\n",
        "bbox = ''\n",
        "count = 0 \n",
        "while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    # Mengkonvert JS response ke OpenCV Image\n",
        "    img = js_to_image(js_reply[\"img\"])\n",
        "\n",
        "    # Membuat transparan overlay untuk bounding box\n",
        "    bbox_array = np.zeros([512,512,4], dtype=np.uint8)\n",
        "\n",
        "    # Mendeteksi bermasker atau tidak\n",
        "    (locs, preds) = detect_and_predict_mask(img, faceNet, model)\n",
        "    for (box, pred) in zip(locs, preds):\n",
        "\n",
        "\t\t# Membuka kotak dan prediksi\n",
        "      (startX, startY, endX, endY) = box\n",
        "      (mask, withoutMask) = pred\n",
        "\n",
        "\t\t# Menggunakan masker hijau, tidak bermasker merah\n",
        "      label = \"Bermasker\" if mask > withoutMask else \"Tidak Bermasker\"\n",
        "      color = (0, 255, 0) if label == \"Bermasker\" else (255, 0, 0)\n",
        "\n",
        "\t\t# Probabilitas pada label\n",
        "      label = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100)\n",
        "\n",
        "\t\t# Menampilkan hasil dengan label dan kotak dari frame\n",
        "      bbox_array=cv2.putText(bbox_array, label, (startX, startY - 10),cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
        "      bbox_array=cv2.rectangle(bbox_array, (startX, startY), (endX, endY), color, 2)\n",
        "\n",
        "    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n",
        "\n",
        "    # Mengkonversi overlay bbox ke dalam bytes\n",
        "    bbox_bytes = bbox_to_bytes(bbox_array)\n",
        " \n",
        "    # Mengupdate bbox ke pada frame selanjutnya untuk mendapat overlay\n",
        "    bbox = bbox_bytes"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}